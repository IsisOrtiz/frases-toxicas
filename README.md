# frases-toxicas

# PyTorch Toxicity Classifier

Este repositório contém um exemplo de um classificador de toxicidade implementado usando PyTorch. O modelo é treinado para classificar sentenças como tóxicas ou não tóxicas.

## Requisitos

Certifique-se de ter o Python instalado, juntamente com as seguintes bibliotecas:

pip install torch nltk

#Observação
Após baixar os arquivos , crie o ambiente com o comando python -m venv venv 
Instale as dependencias do NLTK e PYtorch 
Execute o comando (venv) python main.py 

Python 3.11.6
